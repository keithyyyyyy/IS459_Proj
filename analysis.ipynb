{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Market Basket Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install efficient-apriori\n",
    "!pip3 install pymongo\n",
    "!pip3 install pandas\n",
    "!pip3 install matplotlib\n",
    "!pip3 install seaborn\n",
    "!pip3 install boto3\n",
    "!pip3 install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from efficient_apriori import apriori\n",
    "from IPython.display import display\n",
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "access_key_id = os.getenv(\"ACCESS_KEY_ID\")\n",
    "secret_access_key = os.getenv(\"SECRET_ACCESS_KEY\")\n",
    "\n",
    "bucket_name = 'is459-t3-job-transformed-data'\n",
    "google_object_key = 'google_transformed/google.csv'\n",
    "nodeflair_object_key = 'nodeflair_transformed/jobs.csv'\n",
    "\n",
    "s3 = boto3.client('s3', aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)\n",
    "\n",
    "\n",
    "jobs_df = pd.read_csv(s3.get_object(Bucket=bucket_name, Key=nodeflair_object_key)['Body'])\n",
    "google_df = pd.read_csv(s3.get_object(Bucket=bucket_name, Key=google_object_key)['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = jobs_df.iloc[: , 1:]\n",
    "print(jobs_df.shape)\n",
    "google_df = google_df.iloc[: , 1:]\n",
    "print(google_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [jobs_df, google_df]\n",
    "jobs_df = pd.concat(frames)\n",
    "jobs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(string):\n",
    "    return ast.literal_eval(string)\n",
    "\n",
    "jobs_df['stacks'] = jobs_df['stacks'].apply(str_to_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for index, row in jobs_df.iterrows():\n",
    "    records.append([stack for stack in row['stacks']])\n",
    "\n",
    "print(len(records))\n",
    "\n",
    "print(records[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsets, rules = apriori(records, min_support=0.01, min_confidence=0.5, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_apr_df = pd.DataFrame(columns=['antecedent', 'consequent', 'basket_rule', 'support', 'confidence', 'lift'])\n",
    "for rule in rules:\n",
    "    antecedent = list(rule.lhs)\n",
    "    consequent = list(rule.rhs)\n",
    "    basket_rule = str(antecedent) + \"->\" + str(consequent)\n",
    "    support = rule.support\n",
    "    confidence = rule.confidence\n",
    "    lift = rule.lift\n",
    "    result_apr_df = result_apr_df.append({'antecedent': antecedent, 'consequent': consequent, 'basket_rule': basket_rule, 'support': support, 'confidence': confidence, 'lift': lift}, ignore_index=True)\n",
    "\n",
    "result_apr_df = result_apr_df.sort_values(by=['confidence'], ascending=False)\n",
    "result_apr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_apr_df = result_apr_df.rename_axis('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_apr_df.to_csv('results.csv')\n",
    "result_apr_df.iloc[:100, :].to_csv('results.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CSV files into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB\n",
    "mongo_user = os.getenv(\"MONGO_USER\")\n",
    "mongo_pwd = os.getenv(\"MONGO_PWD\")\n",
    "client = MongoClient(f'mongodb://{mongo_user}:{mongo_pwd}@35.171.48.20:27017')\n",
    "db = client['IS459']\n",
    "collection = db['market_basket_analysis']\n",
    "\n",
    "# Open CSV file and read rows into a list\n",
    "with open('results.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    rows = list(reader)\n",
    "\n",
    "# Define the filter to check if the document already exists\n",
    "for i in range(100):\n",
    "    row = rows[i]\n",
    "    # Regular expression to match any word between single quotes\n",
    "    quote_pattern = re.compile(r\"'([^']*)'\")\n",
    "\n",
    "    # Find all matches of the quote pattern\n",
    "    words_list = quote_pattern.findall(row['antecedent'] + row['consequent'])\n",
    "    # print(words_list)\n",
    "    \n",
    "    new_row = {'index': row['index'], 'stack': words_list, 'stack_count': len(words_list), 'support': row['support'], 'confidence': row['confidence'], 'lift': row['lift']}\n",
    "    # print(new_row)\n",
    "    \n",
    "    filter = {'stack': words_list}\n",
    "    \n",
    "    # # Define the update operation to set the document if it doesn't exist\n",
    "    update = {'$setOnInsert': new_row}\n",
    "    \n",
    "    # # Execute the update operation with upsert=True to insert the document if it doesn't exist\n",
    "    result = collection.update_one(filter, update, upsert=True)\n",
    "    \n",
    "    if result.upserted_id is not None:\n",
    "        print(f\"Document for antecedent: {row['antecedent']} and consequent: {row['consequent']} inserted.\")\n",
    "    else:\n",
    "        print(f\"Document for antecedent: {row['antecedent']} and consequent: {row['consequent']} already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacks_df = pd.DataFrame(columns=['stack', 'count'])\n",
    "stacks_map = {}\n",
    "\n",
    "for row in jobs_df['stacks']:\n",
    "    for item in row:\n",
    "        if item not in stacks_map:\n",
    "            stacks_map[item] = 1\n",
    "        else:\n",
    "            stacks_map[item] += 1\n",
    "for stack, count in stacks_map.items():\n",
    "    stacks_df = stacks_df.append({'stack': stack, 'count': count}, ignore_index=True)\n",
    "\n",
    "stacks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_stacks = db['stacks']\n",
    "\n",
    "stack_records = stacks_df.to_dict('records')\n",
    "for stack in stack_records:\n",
    "    # print(stack['stack'])\n",
    "\n",
    "    # Check if the document exists\n",
    "    filter_condition = {\"stack\": stack[\"stack\"]}\n",
    "    existing_doc = collection_stacks.find_one(filter_condition)\n",
    "\n",
    "    # If the document doesn't exist, insert it\n",
    "    if existing_doc is None:\n",
    "        collection_stacks.insert_one(stack)\n",
    "        print(\"Document inserted.\")\n",
    "    # update the count value of the stack\n",
    "    else:\n",
    "        newvalue = { \"$set\": { 'count': stack['count'] } }\n",
    "        collection_stacks.update_one(filter_condition,newvalue)\n",
    "        print(\"Document updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
